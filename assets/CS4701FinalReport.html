<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_w743tjfna410-8>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-8,lower-roman) ". "}.lst-kix_w743tjfna410-7>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-7,lower-latin) ". "}ul.lst-kix_gtrsq1sd2j2v-8{list-style-type:none}ul.lst-kix_gtrsq1sd2j2v-6{list-style-type:none}.lst-kix_w743tjfna410-2>li{counter-increment:lst-ctn-kix_w743tjfna410-2}ul.lst-kix_gtrsq1sd2j2v-7{list-style-type:none}.lst-kix_w743tjfna410-0>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-0,decimal) ". "}ol.lst-kix_w743tjfna410-1.start{counter-reset:lst-ctn-kix_w743tjfna410-1 0}.lst-kix_kjchicfyyfa1-0>li:before{content:"\0025cf  "}ol.lst-kix_w743tjfna410-6.start{counter-reset:lst-ctn-kix_w743tjfna410-6 0}.lst-kix_w743tjfna410-1>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-1,lower-latin) ". "}.lst-kix_w743tjfna410-2>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-2,lower-roman) ". "}.lst-kix_w743tjfna410-3>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-3,decimal) ". "}.lst-kix_w743tjfna410-5>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-5,lower-roman) ". "}.lst-kix_w743tjfna410-4>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-4,lower-latin) ". "}.lst-kix_w743tjfna410-6>li:before{content:"" counter(lst-ctn-kix_w743tjfna410-6,decimal) ". "}.lst-kix_q9rcqp4s5kqv-0>li:before{content:"\0025cf  "}ul.lst-kix_h20joywmjw56-8{list-style-type:none}ul.lst-kix_h20joywmjw56-7{list-style-type:none}.lst-kix_q9rcqp4s5kqv-1>li:before{content:"\0025cb  "}.lst-kix_q9rcqp4s5kqv-4>li:before{content:"\0025cb  "}ul.lst-kix_h20joywmjw56-4{list-style-type:none}ul.lst-kix_h20joywmjw56-3{list-style-type:none}ul.lst-kix_h20joywmjw56-6{list-style-type:none}ul.lst-kix_h20joywmjw56-5{list-style-type:none}.lst-kix_q9rcqp4s5kqv-2>li:before{content:"\0025a0  "}ul.lst-kix_h20joywmjw56-0{list-style-type:none}.lst-kix_q9rcqp4s5kqv-3>li:before{content:"\0025cf  "}ul.lst-kix_h20joywmjw56-2{list-style-type:none}ul.lst-kix_h20joywmjw56-1{list-style-type:none}.lst-kix_q9rcqp4s5kqv-8>li:before{content:"\0025a0  "}.lst-kix_q9rcqp4s5kqv-5>li:before{content:"\0025a0  "}.lst-kix_w743tjfna410-4>li{counter-increment:lst-ctn-kix_w743tjfna410-4}.lst-kix_q9rcqp4s5kqv-6>li:before{content:"\0025cf  "}.lst-kix_q9rcqp4s5kqv-7>li:before{content:"\0025cb  "}.lst-kix_kjchicfyyfa1-1>li:before{content:"\0025cb  "}ul.lst-kix_kjchicfyyfa1-7{list-style-type:none}ul.lst-kix_kjchicfyyfa1-8{list-style-type:none}.lst-kix_kjchicfyyfa1-2>li:before{content:"\0025a0  "}ul.lst-kix_kjchicfyyfa1-3{list-style-type:none}ul.lst-kix_kjchicfyyfa1-4{list-style-type:none}.lst-kix_kjchicfyyfa1-3>li:before{content:"\0025cf  "}ul.lst-kix_kjchicfyyfa1-5{list-style-type:none}ul.lst-kix_kjchicfyyfa1-6{list-style-type:none}.lst-kix_kjchicfyyfa1-4>li:before{content:"\0025cb  "}.lst-kix_kjchicfyyfa1-5>li:before{content:"\0025a0  "}.lst-kix_kjchicfyyfa1-8>li:before{content:"\0025a0  "}ul.lst-kix_gtrsq1sd2j2v-4{list-style-type:none}ul.lst-kix_gtrsq1sd2j2v-5{list-style-type:none}ul.lst-kix_gtrsq1sd2j2v-2{list-style-type:none}ul.lst-kix_gtrsq1sd2j2v-3{list-style-type:none}.lst-kix_kjchicfyyfa1-6>li:before{content:"\0025cf  "}ul.lst-kix_gtrsq1sd2j2v-0{list-style-type:none}ul.lst-kix_gtrsq1sd2j2v-1{list-style-type:none}.lst-kix_kjchicfyyfa1-7>li:before{content:"\0025cb  "}ul.lst-kix_kjchicfyyfa1-0{list-style-type:none}ul.lst-kix_kjchicfyyfa1-1{list-style-type:none}ul.lst-kix_kjchicfyyfa1-2{list-style-type:none}ul.lst-kix_h688n6otlewa-7{list-style-type:none}ul.lst-kix_h688n6otlewa-8{list-style-type:none}.lst-kix_okh2nzm96q61-7>li:before{content:"-  "}ul.lst-kix_h688n6otlewa-5{list-style-type:none}ul.lst-kix_h688n6otlewa-6{list-style-type:none}.lst-kix_w743tjfna410-8>li{counter-increment:lst-ctn-kix_w743tjfna410-8}ul.lst-kix_h688n6otlewa-3{list-style-type:none}ul.lst-kix_h688n6otlewa-4{list-style-type:none}.lst-kix_okh2nzm96q61-5>li:before{content:"-  "}.lst-kix_h20joywmjw56-8>li:before{content:"-  "}.lst-kix_h688n6otlewa-5>li:before{content:"-  "}.lst-kix_h688n6otlewa-1>li:before{content:"-  "}.lst-kix_h20joywmjw56-6>li:before{content:"-  "}.lst-kix_h688n6otlewa-7>li:before{content:"-  "}ol.lst-kix_w743tjfna410-8{list-style-type:none}ol.lst-kix_w743tjfna410-7{list-style-type:none}ul.lst-kix_okh2nzm96q61-7{list-style-type:none}ol.lst-kix_w743tjfna410-6{list-style-type:none}ul.lst-kix_okh2nzm96q61-8{list-style-type:none}ol.lst-kix_w743tjfna410-5{list-style-type:none}ul.lst-kix_okh2nzm96q61-5{list-style-type:none}ol.lst-kix_w743tjfna410-4{list-style-type:none}ul.lst-kix_okh2nzm96q61-6{list-style-type:none}ol.lst-kix_w743tjfna410-3{list-style-type:none}ol.lst-kix_w743tjfna410-2{list-style-type:none}ol.lst-kix_w743tjfna410-1{list-style-type:none}ol.lst-kix_w743tjfna410-0{list-style-type:none}ul.lst-kix_h688n6otlewa-1{list-style-type:none}.lst-kix_el177wpw03n7-8>li:before{content:"\0025a0  "}ul.lst-kix_h688n6otlewa-2{list-style-type:none}ul.lst-kix_okh2nzm96q61-0{list-style-type:none}ul.lst-kix_h688n6otlewa-0{list-style-type:none}.lst-kix_h688n6otlewa-3>li:before{content:"-  "}ul.lst-kix_okh2nzm96q61-3{list-style-type:none}ol.lst-kix_w743tjfna410-0.start{counter-reset:lst-ctn-kix_w743tjfna410-0 0}ul.lst-kix_okh2nzm96q61-4{list-style-type:none}ul.lst-kix_okh2nzm96q61-1{list-style-type:none}ul.lst-kix_okh2nzm96q61-2{list-style-type:none}.lst-kix_el177wpw03n7-4>li:before{content:"\0025cb  "}ol.lst-kix_w743tjfna410-3.start{counter-reset:lst-ctn-kix_w743tjfna410-3 0}.lst-kix_el177wpw03n7-6>li:before{content:"\0025cf  "}.lst-kix_h20joywmjw56-0>li:before{content:"-  "}.lst-kix_odqadn1y43oz-6>li:before{content:"\0025cf  "}.lst-kix_odqadn1y43oz-8>li:before{content:"\0025a0  "}.lst-kix_h20joywmjw56-4>li:before{content:"-  "}.lst-kix_el177wpw03n7-0>li:before{content:"\0025cf  "}.lst-kix_h20joywmjw56-2>li:before{content:"-  "}.lst-kix_el177wpw03n7-2>li:before{content:"\0025a0  "}.lst-kix_odqadn1y43oz-0>li:before{content:"\0025cf  "}.lst-kix_odqadn1y43oz-2>li:before{content:"\0025a0  "}.lst-kix_odqadn1y43oz-4>li:before{content:"\0025cb  "}ol.lst-kix_w743tjfna410-2.start{counter-reset:lst-ctn-kix_w743tjfna410-2 0}.lst-kix_okh2nzm96q61-1>li:before{content:"-  "}.lst-kix_w743tjfna410-3>li{counter-increment:lst-ctn-kix_w743tjfna410-3}.lst-kix_okh2nzm96q61-3>li:before{content:"-  "}.lst-kix_81mlcl5h1m6u-1>li:before{content:"-  "}.lst-kix_81mlcl5h1m6u-2>li:before{content:"-  "}.lst-kix_81mlcl5h1m6u-3>li:before{content:"-  "}.lst-kix_81mlcl5h1m6u-4>li:before{content:"-  "}ul.lst-kix_el177wpw03n7-5{list-style-type:none}.lst-kix_81mlcl5h1m6u-7>li:before{content:"-  "}ul.lst-kix_el177wpw03n7-6{list-style-type:none}ul.lst-kix_el177wpw03n7-7{list-style-type:none}ul.lst-kix_el177wpw03n7-8{list-style-type:none}ul.lst-kix_el177wpw03n7-1{list-style-type:none}.lst-kix_81mlcl5h1m6u-5>li:before{content:"-  "}ul.lst-kix_el177wpw03n7-2{list-style-type:none}ul.lst-kix_el177wpw03n7-3{list-style-type:none}.lst-kix_81mlcl5h1m6u-6>li:before{content:"-  "}ul.lst-kix_el177wpw03n7-4{list-style-type:none}.lst-kix_w743tjfna410-1>li{counter-increment:lst-ctn-kix_w743tjfna410-1}.lst-kix_81mlcl5h1m6u-8>li:before{content:"-  "}ul.lst-kix_el177wpw03n7-0{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-7{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-8{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-5{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-6{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-3{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-4{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-1{list-style-type:none}ul.lst-kix_q9rcqp4s5kqv-2{list-style-type:none}.lst-kix_81mlcl5h1m6u-0>li:before{content:"-  "}ul.lst-kix_q9rcqp4s5kqv-0{list-style-type:none}.lst-kix_w743tjfna410-5>li{counter-increment:lst-ctn-kix_w743tjfna410-5}.lst-kix_gtrsq1sd2j2v-0>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-5>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-4>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-2>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-3>li:before{content:"-  "}ol.lst-kix_w743tjfna410-4.start{counter-reset:lst-ctn-kix_w743tjfna410-4 0}.lst-kix_gtrsq1sd2j2v-1>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-6>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-7>li:before{content:"-  "}.lst-kix_gtrsq1sd2j2v-8>li:before{content:"-  "}ol.lst-kix_w743tjfna410-5.start{counter-reset:lst-ctn-kix_w743tjfna410-5 0}.lst-kix_okh2nzm96q61-8>li:before{content:"-  "}ul.lst-kix_81mlcl5h1m6u-5{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-4{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-3{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-2{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-8{list-style-type:none}.lst-kix_okh2nzm96q61-6>li:before{content:"-  "}ul.lst-kix_81mlcl5h1m6u-7{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-6{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-1{list-style-type:none}ul.lst-kix_81mlcl5h1m6u-0{list-style-type:none}.lst-kix_h688n6otlewa-4>li:before{content:"-  "}.lst-kix_h688n6otlewa-6>li:before{content:"-  "}.lst-kix_h20joywmjw56-5>li:before{content:"-  "}.lst-kix_h688n6otlewa-8>li:before{content:"-  "}.lst-kix_w743tjfna410-7>li{counter-increment:lst-ctn-kix_w743tjfna410-7}.lst-kix_h688n6otlewa-0>li:before{content:"-  "}.lst-kix_h20joywmjw56-7>li:before{content:"-  "}.lst-kix_h688n6otlewa-2>li:before{content:"-  "}.lst-kix_el177wpw03n7-3>li:before{content:"\0025cf  "}ul.lst-kix_odqadn1y43oz-8{list-style-type:none}ul.lst-kix_odqadn1y43oz-5{list-style-type:none}.lst-kix_el177wpw03n7-7>li:before{content:"\0025cb  "}ul.lst-kix_odqadn1y43oz-4{list-style-type:none}ul.lst-kix_odqadn1y43oz-7{list-style-type:none}ul.lst-kix_odqadn1y43oz-6{list-style-type:none}ul.lst-kix_odqadn1y43oz-1{list-style-type:none}.lst-kix_el177wpw03n7-5>li:before{content:"\0025a0  "}ul.lst-kix_odqadn1y43oz-0{list-style-type:none}ol.lst-kix_w743tjfna410-7.start{counter-reset:lst-ctn-kix_w743tjfna410-7 0}ul.lst-kix_odqadn1y43oz-3{list-style-type:none}ul.lst-kix_odqadn1y43oz-2{list-style-type:none}.lst-kix_odqadn1y43oz-7>li:before{content:"\0025cb  "}.lst-kix_odqadn1y43oz-5>li:before{content:"\0025a0  "}.lst-kix_h20joywmjw56-1>li:before{content:"-  "}.lst-kix_el177wpw03n7-1>li:before{content:"\0025cb  "}.lst-kix_h20joywmjw56-3>li:before{content:"-  "}.lst-kix_odqadn1y43oz-1>li:before{content:"\0025cb  "}.lst-kix_odqadn1y43oz-3>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_okh2nzm96q61-0>li:before{content:"-  "}.lst-kix_okh2nzm96q61-4>li:before{content:"-  "}.lst-kix_w743tjfna410-0>li{counter-increment:lst-ctn-kix_w743tjfna410-0}ol.lst-kix_w743tjfna410-8.start{counter-reset:lst-ctn-kix_w743tjfna410-8 0}.lst-kix_w743tjfna410-6>li{counter-increment:lst-ctn-kix_w743tjfna410-6}.lst-kix_okh2nzm96q61-2>li:before{content:"-  "}ol{margin:0;padding:0}table td,table th{padding:0}.c1{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c5{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Times New Roman";font-style:normal}.c3{background-color:#ffffff;color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c12{color:#b42419;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Times New Roman";font-style:normal}.c20{padding-top:18pt;padding-bottom:6pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c10{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c14{padding-top:20pt;padding-bottom:6pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c16{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Times New Roman";font-style:normal}.c35{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c22{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c11{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-family:"Times New Roman"}.c23{color:#400bd9;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c28{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10pt;font-style:italic}.c31{color:#000000;text-decoration:none;vertical-align:baseline;font-size:14pt;font-style:normal}.c29{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c32{color:#000000;text-decoration:none;vertical-align:baseline;font-size:15pt;font-style:normal}.c27{padding-top:0pt;padding-bottom:0pt;line-height:1.5;text-align:left}.c36{padding-top:0pt;padding-bottom:0pt;line-height:1.5;text-align:center}.c6{background-color:#ffffff;font-family:"Times New Roman";font-weight:400}.c17{font-family:"Times New Roman";font-style:italic;font-weight:400}.c24{margin-left:36pt;padding-left:0pt}.c18{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c25{font-weight:700;font-family:"Times New Roman"}.c21{font-weight:400;font-family:"Times New Roman"}.c30{font-size:10pt;font-style:italic}.c34{font-weight:400;font-family:"Arial"}.c26{padding:0;margin:0}.c13{color:inherit;text-decoration:inherit}.c33{margin-left:36pt}.c7{background-color:#ffffff}.c19{font-size:10.5pt}.c8{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c7 c18 doc-content"><p class="c9"><span class="c10">CS 4701 Final Report - Competitive 2048</span></p><p class="c9"><span class="c15">Hangyu Zhou, Yi Liu, Chenrui Gu</span></p><p class="c9 c8"><span class="c10"></span></p><h1 class="c14" id="h.ksyu612uqrmh"><span class="c16">1. Introduction</span></h1><p class="c0"><span class="c2 c7">The interactive game 2048 by Gabrielle Circulli has gained huge popularity in recent years [5]. It is easy to learn and fun to play with. But due to the limited board size and move options, the game became very predictable. A lot of people became less passionate about this game. In this project, we brought 2048 to the next level by introducing the novel dual-player zero-sum game mode. We call our new game &ldquo;Competitive 2048&rdquo; since the game involves two players competing with each other to get more points. More specifically, in Competitive 2048, two players take turns making their moves. When &nbsp;the same tiles are merged together, the score will be added to the player who made that move. The game ends when there is no valid move left. The player with the higher score wins the game. Different from the original 2048, the goal for each player in Competitive 2048 is not only to gain as many points as possible, but also to &ldquo;interfere&rdquo; with another player&rsquo;s move, making sure when the game ends, they get more points than their competitor. This new game design introduces new dynamics into the renowned game, making the game less predictable and more fun to play.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c6">To make sure our users can enjoy the game when they cannot find a friend to play with, we decided to develop a robust AI agent to play with users. We did some research on the prior work of 2048 as well as other competitive games that use AI. We found that reinforcement learning models have been widely used on zero-sum games like Go, Chess, etc. Their huge success drew our attention and we </span><span class="c6">decided to build our AI system on top of a well-known RL framework called AlphaZero [2] from DeepMind. In this project, we created an environment dedicated to Competitive 2048 for model training. We trained the model based on AlphaZero&rsquo;s self-play pipeline. Finally, we developed a GUI interface for better user experience and deployed the model to our application. </span></p><h1 class="c14" id="h.7fbw7xjnskck"><span class="c16">2. Prior Work</span></h1><p class="c0"><span class="c2 c7">Based on our research before doing the project. There has been some AI implementation for the original 2048 game, which is to train an AI to play 2048 by itself. However, we were not able to find any existing AI that is able to play a competitive 2048 game. Although to our knowledge, our idea of competitive 2048 has never been implemented by anyone, the AI components that we used have been involved in a lot of prior work, especially in training AIs for games.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c2 c7">Since the invention of DeepBlue, AI has started to showcase its tremendous potential on the game playing of many two-player adversarial games. Reinforcement Learning models have been developed for such games and the performance of those models are phenomenal. They can often beat human experts in the specific domain.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c6">R</span><span class="c6">ecently, more research has been done on a more generic framework that handles all two-player competitive games, called AlphaZero. </span><span class="c6">The AlphaZero framework is designed to be flexible and has already shown its success in games like Othello, Chess, and even one of the most difficult board games, Go. Specifically, the AlphaZero framework can train a high-performance model completely through self-playing, without the need of domain-specific knowledge. In this project, we decided to utilize a general portable AlphaZero framework (2016, </span><span class="c6">Thakoor, Shantanu and Nair, Surag and Jhunjhunwala, Megha) to implement our reinforcement learning model.</span></p><h1 class="c14" id="h.2dgm8itthmv4"><span class="c16">3. Methods</span></h1><h2 class="c20" id="h.pjfqscyksf4g"><span class="c10">3.1. Environment</span></h2><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 440.50px; height: 168.51px;"><img alt="" src="images/image3.png" style="width: 440.50px; height: 168.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6 c30">Figure 1. A classic reinforcement learning feedback loop.</span></p><p class="c0"><span class="c2 c7">Under reinforcement learning settings, agents interact with the environment by making actions and receiving rewards. As times go on, it learns to better navigate in that environment to maximize the reward it gains. </span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c6">We constructed a training environment for our model, including how the game state is represented and how the states are being transformed. Since we&rsquo;ve modified the original set of game rules for 2048 to make it competitive, we had to define new game logics. We started by defining the game states and actions of our game. Then we went on implementing some basic game rules such as combining tiles and gaining scores. To facilitate the training later, we also wrote functions that check the valid moves and calculate the next state and scores given an action. </span></p><h2 class="c20" id="h.pfdyi32svq5b"><span class="c10">3.2. Training Pipeline</span></h2><p class="c0"><span class="c6">Next, we went on to the training part of our project. We utilized different components in the general AlphaZero framework [7] and made them suitable for our use case. From there, we built a pipeline that conducts the training process and helps us to refine our model.</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 554.00px; height: 311.00px;"><img alt="" src="images/image7.png" style="width: 598.00px; height: 360.49px; margin-left: -44.00px; margin-top: -49.49px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c22"><span class="c2 c7">Figure 2. </span></p><p class="c0"><span class="c2 c7">For our entire training process, we ran N iterations of training. For every iteration, we generated M games of self-play, where each game of self-play is called an &ldquo;episode&rdquo;. Then for each of the episodes, we created a new MCTS tree. Nodes in the tree represent states of the game and the edges represent the possible actions that we can take from a certain state. We did a number of simulations for each of the turns of the game. During one simulation, we started from the node that represents the current state of the game. Then we selected an action that, together with the state, had the highest Upper Confidence Bound. The Upper Confidence Bound of an action-state pair is calculated using the following formula:</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 366.50px; height: 58.54px;"><img alt="" src="images/image2.png" style="width: 366.50px; height: 58.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2 c7">U is the upper confidence bound of action a and state s. Q is the expected reward of taking action a at state s. P is the initial policy that we use for conducting the current simulations, and finally, N is the number of times that we chose action a at state s in our current episode. Initially, U is set to infinity for each of the action-state pairs to make sure that the agent explores each of the actions at least once. For the first simulation, we chose an action at random since U for all actions are infinite. By the time that all the actions have been chosen once after a few simulations, we started choosing actions based on the actual U value calculated using the above formula. Each time we selected an action, we ended up in another state. If we had never visited this state before, we add a new node for this state in the tree. Then instead of performing a rollout like the traditional MCTS, we input this new state into our neural network and we would get an output of the expected reward from the neural network. We then propagated the value (reward) back up the tree. Another case that we back propagated the value was when the state was the end state of the game. In this case, we propagated the actual result of the game. However, if we chose an action and moved to a state that had been visited before, then we recursively did the tree search on the node we just moved to . After all the simulations were done, we would get a better policy for the current state of the game. Then we would add this state and the new policy as an example to the list of examples that we keep for the entire training process.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c2 c7">After performing a few simulations for each state of the game for M games, we got an updated list of training examples. Then, we train a new network with the updated list of training examples. Now we would have a new model at this point. We then played the new network with the old network for several games. If the new network achieved a win rate above a certain threshold value, we would update our existing model with this new network. So each iteration we were seeking to further improve our trained model.</span></p><h2 class="c20" id="h.ao83jjofz5qe"><span class="c10">3.3. Data Collection</span></h2><p class="c0"><span class="c2 c7">The next step we took was to measure the performance of our model by applying it to real games. After each training iteration, our trained NNet model was saved into a checkpoint file. We then could load that checkpoint file and use its MCTS to make the moves. We also did some work to measure and visualize how the training loss (PI losses and V losses) changed over time. More will be discussed in the Discussion section.</span></p><h2 class="c20" id="h.khcawqkxllls"><span class="c10">3.4. UI</span></h2><p class="c0"><span class="c6">Th</span><span class="c6">e final step would be</span><span class="c6">&nbsp;to implement our UI and make it interactive and user-friendly. When implementing the UI, we had to make a design choice on how to implement our front-end. We decided to use Python&rsquo;s </span><span class="c6">Tkinter GUI framework</span><span class="c2 c7">&nbsp;[6] as it is easy to pick up and can be integrated well with our neural network model written in Python. In designing our UI, we also paid attention to details so that we could maximize end-user experience. For example, we added a mechanism to avoid users from making more than one move in each turn, so the game will not crash even if the user accidentally touches the keyboard twice in a short time.</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 246.50px; height: 285.61px;"><img alt="" src="images/image16.png" style="width: 246.50px; height: 285.61px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c22"><span class="c2 c7">Figure 3. </span></p><h1 class="c14" id="h.d00elxyliuej"><span class="c16">4. Results</span></h1><p class="c0"><span class="c2 c7">In this section, we present Competitive 2048 gameplay to show how the game works and some unique dynamics/strategies introduced from the zero-sum dual-player game mode.</span></p><h2 class="c20" id="h.2fzb3xdy85zz"><span class="c25">4.1. The Basics</span></h2><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 202.50px; height: 213.81px;"><img alt="" src="images/image11.png" style="width: 202.50px; height: 213.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 203.57px; height: 213.50px;"><img alt="" src="images/image4.png" style="width: 203.57px; height: 213.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 204.50px; height: 214.38px;"><img alt="" src="images/image12.png" style="width: 204.50px; height: 214.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c28 c6">Figure 4. The first round of Competitive 2048. User moved up at the first frame, ended up with the second frame. The agent moved down, merging two &ldquo;2&rdquo; tiles and gained 4 points.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c2 c7">Here we demonstrate the basic rules of Competitive 2048 using the UI we implemented. Figure 4 shows the first round of the game. At the beginning, same as the original 2048, there were two random tiles on the board. The user moved up, ended up with the board layout on the second frame. Then the agent decided to move down, merging two &ldquo;2&rdquo; tiles and gained 4 points. After that, the game control was transferred back to the user and this cycle will continue until the game ends. </span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 204.32px; height: 214.28px;"><img alt="" src="images/image18.png" style="width: 204.32px; height: 214.28px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 202.50px; height: 214.65px;"><img alt="" src="images/image10.png" style="width: 202.50px; height: 214.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 205.15px; height: 215.16px;"><img alt="" src="images/image8.png" style="width: 205.15px; height: 215.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c28 c6">Figure 5. The final round of Competitive 2048. The game ends when there is no valid move left. A &ldquo;GAME OVER&rdquo; window will show up. Based on the results, the game will then either show &ldquo;YOU WON!&rdquo; or &ldquo;YOU LOSE!&rdquo;.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c2 c7">The game ends when there is no valid move left. Same as the original 2048, a move is valid if it can merge at least two tiles or if &nbsp;there is space for the tiles in the direction of the move. As shown in the first frame of Figure 5, no tiles can be merged by moving in any direction, and there is no space left on the board, so the game is over. A &ldquo;GAME OVER&rdquo; window then showed up. The user wins if they have a higher score than the agent, otherwise they lose. In this demo, the user had 296 points while the agent only had 140. So the user won. A &ldquo;YOU WON!&rdquo; window showed up in the last frame. If the user loses, a &ldquo;YOU LOSE!&rdquo; window will appear instead.</span></p><h2 class="c20" id="h.num9ks5s5xfc"><span class="c10">4.2. Strategy: Dodging</span></h2><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 203.15px; height: 214.06px;"><img alt="" src="images/image20.png" style="width: 203.15px; height: 214.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 202.50px; height: 213.58px;"><img alt="" src="images/image19.png" style="width: 202.50px; height: 213.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 205.05px; height: 215.06px;"><img alt="" src="images/image9.png" style="width: 205.05px; height: 215.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c28 c6">Figure 6. A showcase of dodging strategy. From frame 1 to 2, the agent moved down, creating the opportunity for the user to merge two &ldquo;16&rdquo; tiles. The user moves left instead of right, gaining 32 points while dodging from aligning two &ldquo;32&rdquo; tiles together and letting the agent get 64 points.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c2 c7">This section explains a unique strategy in Competitive 2048 called &ldquo;dodging&rdquo;. As the goal for each player is to get more points than their opponent, they need to get points while trying to prevent the opponents from merging tiles. Figure 6 is a demonstration of this strategy. Due to limited training, the agent had not acquired such advanced strategies. Instead of moving up at frame 1 to gain 4 points, it moved down and created the chance for the user to merge two &ldquo;16&rdquo; tiles. At frame 2, the user can merge two &ldquo;16&rdquo; and &ldquo;4&rdquo; tiles by moving either left or right. But if the user moves right, there will be two aligned &ldquo;32&rdquo; tiles. The agent could merge them and get 64 points. Instead, the user chose to move left, gaining the same amount of points while &ldquo;dodging&rdquo; the newly created &ldquo;32&rdquo; tile from aligning with the one on the third row.</span></p><h2 class="c20" id="h.g734obwcarsu"><span class="c10">4.3. Strategy: Sabotaging</span></h2><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 202.50px; height: 212.43px;"><img alt="" src="images/image15.png" style="width: 202.50px; height: 212.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 202.15px; height: 211.96px;"><img alt="" src="images/image6.png" style="width: 202.15px; height: 211.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 201.50px; height: 211.63px;"><img alt="" src="images/image1.png" style="width: 201.50px; height: 211.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6 c30">Figure 7. A demonstration of the &ldquo;sabotaging&rdquo; strategy. Here in the first frame, the user could move up to merge the two &ldquo;4&rdquo; tiles and keep the game going. However, as the user has more points than the agent, they will win if the game ends now. So instead the user chose to move left and actively ended the game.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c6">Here is another interesting strategy specific to our game called &ldquo;sabotaging&rdquo;. </span><span class="c6">When a player is on the upper hand, they can choose to sabotage the game and actively end it. Because the end result is determined by each player&rsquo;s score. If a player has more points than the other, it would be great to end the game as soon as possible. In the first frame of Figure 7, the user had more points than the agent. They have the chance to move up, merge the &ldquo;4&rdquo; tiles and keep the game going. But they chose to move left and actively &ldquo;sabotaged&rdquo; the game. The game was over, and the user won.</span></p><h1 class="c14" id="h.qx8cysoqtj9r"><span class="c16">5. Discussion</span></h1><p class="c0"><span class="c2 c7">Because we&rsquo;re building a game-playing model, it is necessary to measure the performance of our game-playing agent versus other players. Because it is not realistic to test our game-playing program on too many real human players, to accomplish this comparison, we decided to measure its performance by comparing its win rate with that of players that use different game-playing strategies.</span></p><h2 class="c20" id="h.amrw3ar5k471"><span class="c10">5.1. Competing With a Random Player</span></h2><p class="c0"><span class="c6">The first strategy we picked was making decisions completely random. We call it the &ldquo;random player&rdquo;. We played our RL agent with the random player for a few games. The random player randomly chose at each move from the available movement directions, and we recorded the winning rate of our RL agent. Here is the result:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 612.50px; height: 247.74px;"><img alt="" src="images/image14.png" style="width: 612.50px; height: 247.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c22"><span class="c2 c7">Figure 8</span></p><p class="c0"><span class="c2 c7">As we can see in the graph, the win rate is in an increasing trend when we increase our iteration times from 0 to 9. However, there is a sudden decline in iteration 10. There is also some fluctuation at other places (between iteration 4 and 6). We suspect that these fluctuations are due to the stochastic nature of the game of 2048 (random tiles being added to the board). We also suspect that the win rate starting to decrease sharply at iteration 9 is possibly due to overfitting. Therefore, from this graph we can see that higher iteration number doesn&rsquo;t necessarily mean a better result, due to natural randomness of the game and the problem of overfitting.</span></p><h2 class="c20" id="h.k1hb61zrzyr"><span class="c10">5.2. Competing With a Greedy Player</span></h2><p class="c0"><span class="c2 c7">Next, we investigated the greedy player. At each move, the greedy player would choose the move that maximizes the score gain. However, it wouldn&rsquo;t analyze potential future moves nor make any predictions - it only focused on the current board state. We believe that this strategy is more closely related to the strategy that most human players would use to play this game. Below is our result:</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 603.50px; height: 259.42px;"><img alt="" src="images/image5.png" style="width: 603.50px; height: 259.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c22"><span class="c2 c7">Figure 9</span></p><p class="c0"><span class="c2 c7">We can see that there are still some fluctuations in the results, which is in fact unavoidable due to the nature of the game. The win rate tends to form a gradually increasing trend as we increase the number of iteration, with an exception at time 6, which we suspect that some random board state came up and made the greedy moves nearly perfect so that it completely outperformed our model(which was still at its initial stage). We are happy to see that in general our model was able to achieve a win rate of around 70% against a greedy player, who plays like an actual human player. This could mean that our model starts to showcase its potential to outperform average human players.</span></p><h2 class="c20" id="h.bwtyj7hm978j"><span class="c10">5.3. PI Losses and V Losses</span></h2><p class="c27"><span class="c21">Our model has a board layout as input, and it outputs two things: (i) a probability distribution over actions which helps to decide what action to take next (policy pi); (ii) an estimated outcome of this state (value v).</span><span class="c25 c7">&nbsp;</span><span class="c2">To gain more insights of our model&rsquo;s performance, we collected our model&rsquo;s loss value during the training session. The data was collected during the period of 10 iterations. Each iteration consisted of 10 episodes. Therefore following graphs display the changes of losses in value estimation and policy over the period of 100 episodes.</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 268.00px;"><img alt="" src="images/image17.png" style="width: 624.00px; height: 268.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c22"><span class="c6">Figure 10</span></p><p class="c27"><span class="c2">There are two noticeable patterns in this loss graph. The first pattern is that there seems to be a spike at the beginning of every iteration (every 10 episodes). We think that this is because we were adding new examples to the example pool in every before training the network in every iteration. This resulted in the network having to predict some examples that it has never seen before. Therefore there is a slight increase in loss at the beginning of each iteration. The second noticeable trend is that the V loss overall is decreasing over the period of 100 episodes. This implies that our model is gradually fitting the distribution. Together with the gradually increasing winning rate of our model when competing with a greedy player that we discussed before, we think that our model is handling the game better after each training iteration..</span></p><p class="c36"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 268.00px;"><img alt="" src="images/image13.png" style="width: 624.00px; height: 268.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c36"><span class="c2 c7">Figure 11</span></p><p class="c0"><span class="c2 c7">The above graph shows the trend of Pi losses over the period of 100 episodes. The overall trend is very similar to that of the V loss graph. However, the spikes are not that obvious compared to the V loss graph. This might be because Pi is a vector and we are taking the average loss of each of the elements, and therefore the line in the graph is more smooth than the line of the V loss graph. Similar to the V loss graph, the overall decreasing trend of the Pi loss graph indicates that our model is outputting better policy over the period of the training process.</span></p><h1 class="c14" id="h.4xx09xd2y1ut"><span class="c16">6. Conclusion</span></h1><p class="c0"><span class="c6">In conclusion, through the use of the AlphaZero framework, we&rsquo;ve been able to produce a successful AI agent for Competitive 2048. The training of the AI model requires no previous domain knowledge and is completely through self-play. We are happy to see that the model can be trained to improve its performance solely through self-play, just like other two-player adversarial games that AlphaZero is good at.</span></p><h2 class="c20" id="h.i4aiiw3xaaov"><span class="c10">6.1. Key Findings</span></h2><p class="c27"><span class="c2">The first thing we found during our project is the importance of the Neural Network in Alphazero. The traditional way of performing MCTS is to have a rollout whenever we encounter a leaf node. The rollout is based on some random moves until the end of the game. However, in Alphazero, we use Neural Network in replace of the rollout part. This not only makes the process faster, but also takes into account some past experience since the network is trained using past examples.</span></p><p class="c27 c8"><span class="c2"></span></p><p class="c27"><span class="c21">Another important thing that we noticed while doing our project is that not every game can be implemented using Alphazeo. Games where each player has the same goal when looking from their own perspectives, and shared resources with each other is the basic requirement for Alphazero to work. Our original Competitive 2048 design was that the human player&rsquo;s goal is to sabotage the agent and make it die as fast as possible. However, during implementation we quickly realized that it was impossible for us to adopt Alphazero to train the agent since different players have different goals. This made it impossible to train a single neural network and make the trained network compete itself. So we changed the rule a little bit so that players have aligned goals. This realization of the fundamental difference between games enhanced our understanding of how Alphazero works.</span></p><h2 class="c20" id="h.gf7tgmm5dyei"><span class="c10">6.2. Future Directions</span></h2><p class="c0"><span class="c2 c7">If we were to continue our work in the future, we can spend more time on correctly handling the stochasticity of the 2048 game (which is the most unique part of the game). In a traditional 2048 game, there will be random tiles showing up at random positions. This tends to make the game non-deterministic compared to other two-player adversarial games like Go, in which one move can only lead to one potential next state. In our case, one move can potentially lead to many other next states, depending on the positions of the newly generated tiles. To handle this stochasticity, we would need to figure out ways to mitigate its impact on the training process. As we can see in the performance graphs, there are always fluctuations in the graph due to the randomness of the game.</span></p><p class="c0 c8"><span class="c2 c7"></span></p><p class="c0"><span class="c2 c7">If we have longer time and better devices, we would also like to train our model more intensively with more iterations, episodes and simulations. &nbsp;In addition, we could also test out different hyperparameters for our network to further enhance the performance of our model.</span></p><h1 class="c14" id="h.fdw7ql80fclt"><span class="c16">7. Code</span></h1><p class="c0"><span class="c6">Our code is available at: </span><span class="c11 c7"><a class="c13" href="https://www.google.com/url?q=https://github.coecis.cornell.edu/hz477/Competitive2048&amp;sa=D&amp;source=editors&amp;ust=1671253672263042&amp;usg=AOvVaw3JfRFSAMA66rScXz4RRHW4">https://github.coecis.cornell.edu/hz477/Competitive2048</a></span><span class="c2 c7">. In the file README.md in the root directory, we mentioned the libraries we used as well as indicated which parts of the code were written by our team (we also included this information below in section 7.1). Some basic information about our project is also included in this README.md file. We will likely to further develop our game in the future according to the future work we mentioned in the previous section. The changes will be reflected in this github repository.</span></p><h2 class="c20" id="h.8pr9mkc5qcv7"><span>7.1 Code References</span></h2><p class="c0"><span class="c25">General AlphaZero framework:</span></p><p class="c0"><span class="c21">Adapted from </span><span class="c11"><a class="c13" href="https://www.google.com/url?q=https://github.com/suragnair/alpha-zero-general&amp;sa=D&amp;source=editors&amp;ust=1671253672263844&amp;usg=AOvVaw35DPL8mT59UavDCeu3LnwH">Alpha Zero General</a></span><span class="c21">&nbsp;repository [7].</span></p><ul class="c26 lst-kix_odqadn1y43oz-0 start"><li class="c0 c24 li-bullet-0"><span class="c2">We adapted the framework of the Othello game&#39;s environment in the Alpha Zero General repository and implemented all the functions by ourselves based on our game of 2048 (in the env folder). </span></li><li class="c0 c24 li-bullet-0"><span class="c2">We adapted NNet.py from the Alpha Zero General repository and implemented some additional codes to calculate the value loss and pi loss during training. </span></li><li class="c0 c24 li-bullet-0"><span class="c2">We adapted Coach.py from the Alpha Zero General repository and implemented some additional codes to calculate the winning rate of our model when playing against a random player and a greedy player.</span></li><li class="c0 c24 li-bullet-0"><span class="c2">We adapted the other files related to alphazero from the Alpha Zero General repository and made changes to the code so that it works for competitive 2048</span></li></ul><p class="c0 c33 c8"><span class="c2"></span></p><p class="c0"><span class="c25 c29">UI Implementation:</span></p><p class="c0"><span class="c21">Adapted from </span><span class="c11"><a class="c13" href="https://www.google.com/url?q=https://pythongeeks.org/python-2048-game-source-code/.&amp;sa=D&amp;source=editors&amp;ust=1671253672264845&amp;usg=AOvVaw0VG1PexLw6QDUas2iRpg9d">Create 2048 Game using Python</a></span><span class="c21">&nbsp;</span><span class="c2">website [8].</span></p><ul class="c26 lst-kix_kjchicfyyfa1-0 start"><li class="c0 c24 li-bullet-0"><span class="c2">We adapted the framework of our UI from the above website and implemented all the functions by ourselves based on the logic of our game.</span></li></ul><p class="c0 c8"><span class="c2 c7"></span></p><hr style="page-break-before:always;display:none;"><p class="c0 c8"><span class="c2 c7"></span></p><h2 class="c7 c35" id="h.g2gu4anmdu8j"><span class="c25">7.2. File Structure</span></h2><p class="c9 c7"><span class="c23 c7 c19">.</span></p><p class="c9 c7"><span class="c6 c19">&#9492;&#9472;&#9472; </span><span class="c23 c7 c19">Competitive2048</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9500;&#9472;&#9472; README.md</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9500;&#9472;&#9472; </span><span class="c7 c19 c23">comp2048</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; </span><span class="c23 c7 c19">alphazero</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Arena.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Coach.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Game.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; MCTS.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; NeuralNet.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; README.md</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; pit.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; </span><span class="c23 c7 c19">temp_cpu</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; </span><span class="c7 c12">best.pth.tar</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; train_data_10.json</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9492;&#9472;&#9472; train_data_20.json</span></p><p class="c7 c9"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9492;&#9472;&#9472; train.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; </span><span class="c23 c7 c19">env</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Comp2048Game.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Comp2048Logic.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9492;&#9472;&#9472; </span><span class="c23 c7 c19">legacy</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &nbsp; &nbsp; &#9500;&#9472;&#9472; Comp2048Gym.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &nbsp; &nbsp; &#9500;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &nbsp; &nbsp; &#9492;&#9472;&#9472; test_Comp2048Gym.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; </span><span class="c23 c7 c19">models</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Comp2048NNet.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; Comp2048Players.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; NNet.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9492;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; </span><span class="c23 c7 c19">ui</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; deprecated_game.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9500;&#9472;&#9472; game.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &#9474; &nbsp; &#9492;&#9472;&#9472; user.py</span></p><p class="c9 c7"><span class="c6 c19">&nbsp; &nbsp; &#9474; &nbsp; &#9492;&#9472;&#9472; </span><span class="c23 c7 c19">utils</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &nbsp; &nbsp; &#9500;&#9472;&#9472; __init__.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9474; &nbsp; &nbsp; &nbsp; &#9492;&#9472;&#9472; utils.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9500;&#9472;&#9472; demo.txt</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9500;&#9472;&#9472; graph.py</span></p><p class="c9 c7"><span class="c5">&nbsp; &nbsp; &#9492;&#9472;&#9472; setup.py</span></p><p class="c9 c7"><span class="c5">9 directories, 33 files</span></p><p class="c9 c7 c8"><span class="c5"></span></p><h1 class="c14" id="h.4t06b4ldpbbg"><span class="c16">References</span></h1><ol class="c26 lst-kix_w743tjfna410-0 start" start="1"><li class="c0 c24 li-bullet-0"><span class="c21">Stanford.edu. (2017). </span><span class="c17">Simple Alpha Zero</span><span class="c21">. [online] Available at: </span><span class="c11"><a class="c13" href="https://www.google.com/url?q=https://web.stanford.edu/~surag/posts/alphazero.html&amp;sa=D&amp;source=editors&amp;ust=1671253672270053&amp;usg=AOvVaw0qi33O21-xJh0IVLZKsldK">https://web.stanford.edu/~surag/posts/alphazero.html</a></span><span class="c2">.</span></li><li class="c0 c24 li-bullet-0"><span class="c21">Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K. and Hassabis, D. (n.d.). </span><span class="c17">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</span><span class="c21">. [online] Available at: </span><span class="c11"><a class="c13" href="https://www.google.com/url?q=https://arxiv.org/pdf/1712.01815.pdf&amp;sa=D&amp;source=editors&amp;ust=1671253672270582&amp;usg=AOvVaw3bQ3b8zipTeyfrYV8z_n_9">https://arxiv.org/pdf/1712.01815.pdf</a></span><span class="c2">.</span></li><li class="c0 c24 li-bullet-0"><span class="c21">Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T. and Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. </span><span class="c17">Nature</span><span class="c2">, 529(7587), pp.484&ndash;489. doi:10.1038/nature16961.</span></li><li class="c0 c24 li-bullet-0"><span class="c21">Zhou, Y. (2019). </span><span class="c17">From AlphaGo Zero to 2048</span><span class="c21">. [online] www.semanticscholar.org. Available at: </span><span class="c11"><a class="c13" href="https://www.google.com/url?q=https://www.semanticscholar.org/paper/From-AlphaGo-Zero-to-2048-Zhou/b9cc9a861531ede494a365e42b7ba788d638eaec&amp;sa=D&amp;source=editors&amp;ust=1671253672271315&amp;usg=AOvVaw10b240NHtF5MEirtHgPNWu">https://www.semanticscholar.org/paper/From-AlphaGo-Zero-to-2048-Zhou/b9cc9a861531ede494a365e42b7ba788d638eaec</a></span></li><li class="c0 c24 li-bullet-0"><span class="c21">Kharpal, A. (n.d.). </span><span class="c17">19-year-old makes viral game hit in a weekend</span><span class="c2">. [online] CNBC. Available at: https://www.cnbc.com/2014/03/26/19-year-old-makes-viral-game-hit-in-a-weekend.html</span></li><li class="c0 c24 li-bullet-0"><span class="c21">Python Software Foundation (2019). </span><span class="c17">tkinter &mdash; Python interface to Tcl/Tk &mdash; Python 3.7.2 documentation</span><span class="c2">. [online] python.org. Available at: https://docs.python.org/3/library/tkinter.html.</span></li><li class="c0 c24 li-bullet-0"><span class="c21">Nair, S. (2022). </span><span class="c17">Alpha Zero General (any game, any framework!)</span><span class="c21">. [online] GitHub. Available at: </span><span class="c11"><a class="c13" href="https://www.google.com/url?q=https://github.com/suragnair/alpha-zero-general&amp;sa=D&amp;source=editors&amp;ust=1671253672272086&amp;usg=AOvVaw3ynVUriLUMUQD_BGom2XcB">https://github.com/suragnair/alpha-zero-general</a></span></li><li class="c0 c24 li-bullet-0"><span class="c21">Geeks, P. (2022). </span><span class="c17">Create 2048 Game using Python</span><span class="c2">. [online] Python Geeks. Available at: https://pythongeeks.org/python-2048-game-source-code/.</span></li></ol><p class="c0 c8 c33"><span class="c2"></span></p><p class="c9 c8"><span class="c29 c34"></span></p></body></html>